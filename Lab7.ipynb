{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc100=np.random.choice([0,1],size=8*1024*1024*100,replace=True, p=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc100=np.packbits(perc100)\n",
    "perc90=np.random.choice([0,1],size=8*1024*1024*100,replace=True, p=[.9,.1])\n",
    "perc90=np.packbits(perc90)\n",
    "perc80=np.random.choice([0,1],size=8*1024*1024*100,replace=True, p=[.8,.2])\n",
    "perc80=np.packbits(perc80)\n",
    "perc70=np.random.choice([0,1],size=8*1024*1024*100,replace=True, p=[.7,.3])\n",
    "perc70=np.packbits(perc70)\n",
    "perc60=np.random.choice([0,1],size=8*1024*1024*100,replace=True, p=[.6,.4])\n",
    "perc60=np.packbits(perc60)\n",
    "perc50=np.random.choice([0,1],size=8*1024*1024*100,replace=True, p=[.5,.5])\n",
    "perc50=np.packbits(perc50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104857600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('perc100','wb').write(perc100)\n",
    "open('perc90','wb').write(perc90)\n",
    "open('perc80','wb').write(perc80)\n",
    "open('perc70','wb').write(perc70)\n",
    "open('perc60','wb').write(perc60)\n",
    "open('perc50','wb').write(perc50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNA=np.random.choice(['A','C','T','G'], size=100000000, replace=True, p=[.25,.25,.25,.25])\n",
    "protein=np.random.choice(['G','A','L','M','F','W','K','Q','E','S','P','V','I','V','Y','H','R','N','D','T'], size=100000000, replace=True, p=[.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05,.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open('nt_seq.fa','w').write(''.join(DNA))\n",
    "open('protein_seq.fa','w').write(''.join(protein))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Table1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most files, arithmetic compress was the best algorithm for compression (see table above). However, arithmetic compress takes a significant amount of time and was only marginally better than other methods for the nucleotide and protein sequences. The fastest algorithm was consistently pbzip2. \n",
    "\n",
    "The difference between pbzip2 and bzip2 is that pbzip2 is a parallel version of bzip2, so I expect pbzip2 to be faster, since parallel computing can significantly speed up processing. \n",
    "\n",
    "The level of compression increases as the percentage of zeros increases. This is because when compressing a file, when there is a high percentage of zeros, you can code a repeating sequence of zeros, as x number of zeros, as opposed to zero x times (ie 00000 vs 5 0s). The greater the percentage of zeros, the more you can compress the file.\n",
    "\n",
    "A single DNA base can be stored in 2 bits.\n",
    "\n",
    "An amino acid can be stored in 5 bits.\n",
    "\n",
    "Number of bits to store DNA sequence\n",
    "formula=# of bases/# of bits\n",
    "gzip=29.2MB, or 2.336 bits\n",
    "bzip2= 27.3 MB, or 2.184 bits\n",
    "\n",
    "Number of bits to store Protein sequence\n",
    "gzip=59.5MB, or 4.76 bits\n",
    "bzip2=54.2MB, or 4.336 bits.\n",
    "\n",
    "For DNA sequences, both compression tools were less efficient than the minimum number of bits.\n",
    "\n",
    "For protein sequences, both compression tools were actually more efficient than the minimum number of bits required to encode an amino acid.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "open('fasta.fa','w')\n",
    "Entrez.email='neilwang@berkeley.edu'\n",
    "handle=Entrez.esearch(db='nucleotide',term='gp120 HIV',sort='relevance',idtype='acc')\n",
    "x=0\n",
    "for i in Entrez.read(handle)['IdList']:\n",
    "    if x<11:\n",
    "        handle=Entrez.efetch(db='nucleotide',id=i, rettype='fasta',retmode='text')\n",
    "        open('fasta.fa','a+').write(handle.read())\n",
    "        x=x+1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A priori, I expect this file to achieve better compression than random data, because all these sequences are homologues, thus, they have many parts that are identical, which will lead to better compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HIV gp120 Homolog\n",
    "Original File Size: 6.66kB\n",
    "\n",
    "gzip file size: 897B, compression ratio = .135\n",
    "\n",
    "bzip2 file size: 1.07kB, compression ratio = .16\n",
    "\n",
    "ArithmeticCompress file size: 3.6kB, compression ratio = .54\n",
    "\n",
    "\n",
    "\n",
    "For random Nucleotide sequence\n",
    "\n",
    "gzip compression ratio: .292\n",
    "\n",
    "bzip2 compression ratio: .273\n",
    "\n",
    "ArithmeticCompress compression ratio: .25\n",
    "\n",
    "gzip and bzip2 both performed far better on the HIV gp120 homolog sequences, as compared to a random nucleotide sequence. However, Arithmetic compress performed far worse on the HIV gp120 homolog sequences, as compared to a random nucleotide sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the genomic data, I propose we use gzip. For the protein sequences, I propose we use pbzip2. And for the binary microscope images, I propose we use ArithmeticCompress. ArithmeticCompress is very slow, but since only 10% of the total data is compressed with this method, and given that the compression is very good, the time delay is worth it.\n",
    "\n",
    "Using the compression ratio of .135 as obtained by using gzip in the HIV gp120 file compression, and the compression ratio of .542 as obtained by using pbzip2 for a random protein sequence, and a compression ratio of .25 as obtained by using ArithmeticCompress for a binary sequence with 50% zeros, we would save 812.8 terabytes of data a day or 81.28% savings, or 296672 Terabytes a year. My anticipated bonus would be around $14,833,600!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
